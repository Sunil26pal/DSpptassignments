{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Working with RDDs:\n",
        "   a) Write a Python program to create an RDD from a local data source.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kseo6fxsqajB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Creation Example\")\n",
        "\n",
        "# Path to the local data source\n",
        "data_file = \"path/to/your/local/data.txt\"\n",
        "\n",
        "# Create an RDD from a local data source\n",
        "rdd = sc.textFile(data_file)\n",
        "\n",
        "# Perform transformations and actions on the RDD\n",
        "# For example, print the contents of the RDD\n",
        "for line in rdd.collect():\n",
        "    print(line)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "JbCb18QNsh8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
        "  \n"
      ],
      "metadata": {
        "id": "XQTLOWsvqdJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Processing Example\")\n",
        "\n",
        "# Path to the local data source\n",
        "data_file = \"path/to/your/local/data.txt\"\n",
        "\n",
        "# Create an RDD from a local data source\n",
        "rdd = sc.textFile(data_file)\n",
        "\n",
        "# Transformation: Map\n",
        "mapped_rdd = rdd.map(lambda line: line.upper())\n",
        "\n",
        "# Transformation: Filter\n",
        "filtered_rdd = rdd.filter(lambda line: 'Spark' in line)\n",
        "\n",
        "# Transformation: FlatMap\n",
        "words_rdd = rdd.flatMap(lambda line: line.split())\n",
        "\n",
        "# Action: Collect\n",
        "all_lines = rdd.collect()\n",
        "\n",
        "# Action: Count\n",
        "line_count = rdd.count()\n",
        "\n",
        "# Action: Take\n",
        "sample_lines = rdd.take(5)\n",
        "\n",
        "# Print the transformed RDDs and computed results\n",
        "print(\"Mapped RDD:\")\n",
        "for line in mapped_rdd.collect():\n",
        "    print(line)\n",
        "\n",
        "print(\"Filtered RDD:\")\n",
        "for line in filtered_rdd.collect():\n",
        "    print(line)\n",
        "\n",
        "print(\"Words RDD:\")\n",
        "for word in words_rdd.collect():\n",
        "    print(word)\n",
        "\n",
        "print(\"All Lines:\")\n",
        "for line in all_lines:\n",
        "    print(line)\n",
        "\n",
        "print(\"Line Count:\", line_count)\n",
        "\n",
        "print(\"Sample Lines:\")\n",
        "for line in sample_lines:\n",
        "    print(line)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "pYiLaQKMsqP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate."
      ],
      "metadata": {
        "id": "65oBO5xSqdG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Operations Example\")\n",
        "\n",
        "# Path to the local data source\n",
        "data_file = \"path/to/your/local/data.txt\"\n",
        "\n",
        "# Create an RDD from a local data source\n",
        "rdd = sc.textFile(data_file)\n",
        "\n",
        "# Transformation: Map\n",
        "mapped_rdd = rdd.map(lambda line: line.split(','))\n",
        "\n",
        "# Transformation: Filter\n",
        "filtered_rdd = mapped_rdd.filter(lambda record: int(record[2]) > 30)\n",
        "\n",
        "# Transformation: Reduce\n",
        "total_age = filtered_rdd.map(lambda record: int(record[2])).reduce(lambda a, b: a + b)\n",
        "\n",
        "# Transformation: Aggregate\n",
        "avg_age_count = filtered_rdd.aggregate(\n",
        "    (0, 0),\n",
        "    lambda acc, record: (acc[0] + int(record[2]), acc[1] + 1),\n",
        "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        ")\n",
        "avg_age = avg_age_count[0] / avg_age_count[1]\n",
        "\n",
        "# Print the results\n",
        "print(\"Total Age:\", total_age)\n",
        "print(\"Average Age:\", avg_age)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "nArDqiCyu653"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######################################################"
      ],
      "metadata": {
        "id": "bDKVqMVevO2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Spark DataFrame Operations:\n",
        "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
        "  "
      ],
      "metadata": {
        "id": "UD6bOSORqdBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Load CSV into DataFrame\").getOrCreate()\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_file = \"path/to/your/csv/file.csv\"\n",
        "\n",
        "# Load CSV into DataFrame\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "9JB-XyQUvWub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
        ""
      ],
      "metadata": {
        "id": "gv8SG5pKqc5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrame Operations\").getOrCreate()\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_file = \"path/to/your/csv/file.csv\"\n",
        "\n",
        "# Load CSV into DataFrame\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file)\n",
        "\n",
        "# Filter rows based on a condition\n",
        "filtered_df = df.filter(df[\"Age\"] > 30)\n",
        "\n",
        "# Group by a column and calculate the average of another column\n",
        "grouped_df = df.groupBy(\"Country\").avg(\"Salary\")\n",
        "\n",
        "# Join two DataFrames\n",
        "another_csv_file = \"path/to/another/csv/file.csv\"\n",
        "another_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(another_csv_file)\n",
        "joined_df = df.join(another_df, on=\"ID\", how=\"inner\")\n",
        "\n",
        "# Display the DataFrames\n",
        "print(\"Filtered DataFrame:\")\n",
        "filtered_df.show()\n",
        "\n",
        "print(\"Grouped DataFrame:\")\n",
        "grouped_df.show()\n",
        "\n",
        "print(\"Joined DataFrame:\")\n",
        "joined_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "bDucGHUovi3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "zOtZalVFqc3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Queries\").getOrCreate()\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_file = \"path/to/your/csv/file.csv\"\n",
        "\n",
        "# Load CSV into DataFrame\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file)\n",
        "\n",
        "# Create a temporary view from the DataFrame\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "\n",
        "# Perform Spark SQL queries\n",
        "query1 = \"SELECT COUNT(*) AS TotalCount FROM my_table\"\n",
        "query2 = \"SELECT Country, AVG(Salary) AS AverageSalary FROM my_table GROUP BY Country\"\n",
        "query3 = \"SELECT * FROM my_table WHERE Age > 30\"\n",
        "\n",
        "# Execute the queries and retrieve the results as DataFrames\n",
        "result1 = spark.sql(query1)\n",
        "result2 = spark.sql(query2)\n",
        "result3 = spark.sql(query3)\n",
        "\n",
        "# Display the results\n",
        "print(\"Query 1: Total Count\")\n",
        "result1.show()\n",
        "\n",
        "print(\"Query 2: Average Salary by Country\")\n",
        "result2.show()\n",
        "\n",
        "print(\"Query 3: Records where Age > 30\")\n",
        "result3.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "dTUNrUWzvwva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###################################################"
      ],
      "metadata": {
        "id": "Q6MO3MF7qc0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Spark Streaming:\n",
        "  a) Write a Python program to create a Spark Streaming application.\n",
        "  \n"
      ],
      "metadata": {
        "id": "IaXQQ01YqcxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark Streaming Application\").getOrCreate()\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# Set the log level to only display errors\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Create a DStream by connecting to a TCP socket\n",
        "host = \"localhost\"\n",
        "port = 9999\n",
        "lines = ssc.socketTextStream(host, port)\n",
        "\n",
        "# Process the incoming data stream\n",
        "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "                  .map(lambda word: (word, 1)) \\\n",
        "                  .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming computation\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming to finish\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "Nxak3mjsv7aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
        "  \n"
      ],
      "metadata": {
        "id": "7i6duzA5qcnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark Streaming Application\").getOrCreate()\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# Set the log level to only display errors\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Kafka configuration\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",\n",
        "    \"subscribe\": \"my_topic\"\n",
        "}\n",
        "\n",
        "# Create a DStream by consuming data from Kafka\n",
        "kafka_stream = KafkaUtils.createDirectStream(ssc, topics=[kafka_params[\"subscribe\"]], kafkaParams=kafka_params)\n",
        "\n",
        "# Extract the data from Kafka messages\n",
        "lines = kafka_stream.map(lambda x: x[1])\n",
        "\n",
        "# Process the incoming data stream\n",
        "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "                  .map(lambda word: (word, 1)) \\\n",
        "                  .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming computation\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming to finish\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "NUHL8e4pv67z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " c) Implement streaming transformations and actions to process and analyze the incoming data stream."
      ],
      "metadata": {
        "id": "bkxy8nfqqck6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Streaming Transformations and Actions\").getOrCreate()\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# Set the log level to only display errors\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Kafka configuration\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",\n",
        "    \"subscribe\": \"my_topic\"\n",
        "}\n",
        "\n",
        "# Create a DStream by consuming data from Kafka\n",
        "kafka_stream = KafkaUtils.createDirectStream(ssc, topics=[kafka_params[\"subscribe\"]], kafkaParams=kafka_params)\n",
        "\n",
        "# Extract the data from Kafka messages\n",
        "lines = kafka_stream.map(lambda x: x[1])\n",
        "\n",
        "# Perform streaming transformations and actions\n",
        "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "                  .map(lambda word: (word, 1)) \\\n",
        "                  .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Process each RDD in the stream\n",
        "def process_rdd(time, rdd):\n",
        "    if not rdd.isEmpty():\n",
        "        print(\"RDD processing time:\", time)\n",
        "        print(\"Word counts:\")\n",
        "        for word, count in rdd.collect():\n",
        "            print(f\"{word}: {count}\")\n",
        "        print()\n",
        "\n",
        "# Apply the process_rdd function to each RDD in the DStream\n",
        "word_counts.foreachRDD(process_rdd)\n",
        "\n",
        "# Start the streaming computation\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming to finish\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "4S2FSTJyv8Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###############################################"
      ],
      "metadata": {
        "id": "I282CzkRqciZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Spark SQL and Data Source Integration:\n",
        "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
        ""
      ],
      "metadata": {
        "id": "KHvOx2usqcfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark Database Connection\").getOrCreate()\n",
        "\n",
        "# Database connection properties\n",
        "db_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
        "db_table = \"mytable\"\n",
        "db_user = \"myuser\"\n",
        "db_password = \"mypassword\"\n",
        "\n",
        "# Read data from the database table\n",
        "df = spark.read.format(\"jdbc\") \\\n",
        "    .option(\"url\", db_url) \\\n",
        "    .option(\"dbtable\", db_table) \\\n",
        "    .option(\"user\", db_user) \\\n",
        "    .option(\"password\", db_password) \\\n",
        "    .load()\n",
        "\n",
        "# Perform operations on the DataFrame\n",
        "# For example, display the contents of the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "RKz9cclXv9b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
        "   "
      ],
      "metadata": {
        "id": "f1Y_BOJlqcdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Operations\").getOrCreate()\n",
        "\n",
        "# Database connection properties\n",
        "db_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
        "db_table = \"mytable\"\n",
        "db_user = \"myuser\"\n",
        "db_password = \"mypassword\"\n",
        "\n",
        "# Register the database table as a temporary view\n",
        "spark.read.format(\"jdbc\") \\\n",
        "    .option(\"url\", db_url) \\\n",
        "    .option(\"dbtable\", db_table) \\\n",
        "    .option(\"user\", db_user) \\\n",
        "    .option(\"password\", db_password) \\\n",
        "    .load() \\\n",
        "    .createOrReplaceTempView(\"my_table\")\n",
        "\n",
        "# Perform SQL operations on the data\n",
        "query1 = \"SELECT * FROM my_table WHERE age > 30\"\n",
        "query2 = \"SELECT COUNT(*) AS total_count FROM my_table\"\n",
        "query3 = \"SELECT country, AVG(salary) AS average_salary FROM my_table GROUP BY country\"\n",
        "\n",
        "# Execute the SQL queries and retrieve the results as DataFrames\n",
        "result1 = spark.sql(query1)\n",
        "result2 = spark.sql(query2)\n",
        "result3 = spark.sql(query3)\n",
        "\n",
        "# Display the results\n",
        "print(\"Query 1: Filtered Data\")\n",
        "result1.show()\n",
        "\n",
        "print(\"Query 2: Total Count\")\n",
        "result2.show()\n",
        "\n",
        "print(\"Query 3: Average Salary by Country\")\n",
        "result3.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "TMIl5jhjv-DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3."
      ],
      "metadata": {
        "id": "Z2uf4j1rqcai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoIoxSY_pvAd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark Data Integration\").getOrCreate()\n",
        "\n",
        "# Read data from HDFS\n",
        "hdfs_file = \"hdfs://localhost:9000/path/to/data.parquet\"\n",
        "df = spark.read.format(\"parquet\").load(hdfs_file)\n",
        "\n",
        "# Write data to Amazon S3\n",
        "s3_bucket = \"my-s3-bucket\"\n",
        "s3_path = \"s3a://my-s3-bucket/path/to/data.parquet\"\n",
        "df.write.format(\"parquet\").save(s3_path)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ]
    }
  ]
}