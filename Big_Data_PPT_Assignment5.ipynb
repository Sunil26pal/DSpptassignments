{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Setting up a Kafka Producer:\n",
        "   a) Write a Python program to create a Kafka producer.\n",
        "   \n"
      ],
      "metadata": {
        "id": "MnoOUpkASCuH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kAkUw9DMoYr"
      },
      "outputs": [],
      "source": [
        "from confluent_kafka import Producer\n",
        "\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    if err is not None:\n",
        "        print(f\"Message delivery failed: {err}\")\n",
        "    else:\n",
        "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
        "\n",
        "\n",
        "def produce_kafka_message(producer, topic, message):\n",
        "    producer.produce(topic, message.encode('utf-8'), callback=delivery_report)\n",
        "    producer.poll(0.5)  # Flushes the producer buffer\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka producer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers\n",
        "    }\n",
        "\n",
        "    # Create Kafka producer instance\n",
        "    producer = Producer(conf)\n",
        "\n",
        "    # Produce some messages\n",
        "    messages = [\n",
        "        'Hello, Kafka!',\n",
        "        'This is a test message.',\n",
        "        'Another message for Kafka.'\n",
        "    ]\n",
        "\n",
        "    for message in messages:\n",
        "        produce_kafka_message(producer, topic, message)\n",
        "\n",
        "    # Flush any remaining messages in the producer buffer\n",
        "    producer.flush()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1b) Configure the producer to connect to a Kafka cluster.\n",
        "  "
      ],
      "metadata": {
        "id": "jeyKsXuXS6N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer\n",
        "\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    if err is not None:\n",
        "        print(f\"Message delivery failed: {err}\")\n",
        "    else:\n",
        "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
        "\n",
        "\n",
        "def produce_kafka_message(producer, topic, message):\n",
        "    producer.produce(topic, message.encode('utf-8'), callback=delivery_report)\n",
        "    producer.poll(0.5)  # Flushes the producer buffer\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'kafka1:9092,kafka2:9092,kafka3:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka producer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'client.id': 'my_producer',  # Optional: Assign a client ID for better tracking\n",
        "        'acks': 'all',  # Optional: Set the number of acknowledgments the producer requires\n",
        "        'compression.type': 'gzip'  # Optional: Enable compression for messages\n",
        "        # Add any other desired configuration options here\n",
        "    }\n",
        "\n",
        "    # Create Kafka producer instance\n",
        "    producer = Producer(conf)\n",
        "\n",
        "    # Produce some messages\n",
        "    messages = [\n",
        "        'Hello, Kafka!',\n",
        "        'This is a test message.',\n",
        "        'Another message for Kafka.'\n",
        "    ]\n",
        "\n",
        "    for message in messages:\n",
        "        produce_kafka_message(producer, topic, message)\n",
        "\n",
        "    # Flush any remaining messages in the producer buffer\n",
        "    producer.flush()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "GPUAvL6-S9F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1c) Implement logic to send messages to a Kafka topic.\n"
      ],
      "metadata": {
        "id": "uIOeQ5pFS-RU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*********************************************************************************"
      ],
      "metadata": {
        "id": "f84mhYVuTE-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer\n",
        "\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    if err is not None:\n",
        "        print(f\"Message delivery failed: {err}\")\n",
        "    else:\n",
        "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
        "\n",
        "\n",
        "def send_message(producer, topic, message):\n",
        "    producer.produce(topic, message.encode('utf-8'), callback=delivery_report)\n",
        "    producer.poll(0.5)  # Flushes the producer buffer\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'kafka1:9092,kafka2:9092,kafka3:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka producer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'client.id': 'my_producer',\n",
        "        'acks': 'all',\n",
        "        'compression.type': 'gzip'\n",
        "    }\n",
        "\n",
        "    # Create Kafka producer instance\n",
        "    producer = Producer(conf)\n",
        "\n",
        "    # Send messages to Kafka topic\n",
        "    while True:\n",
        "        message = input(\"Enter a message (or 'exit' to quit): \")\n",
        "        if message.lower() == 'exit':\n",
        "            break\n",
        "        send_message(producer, topic, message)\n",
        "\n",
        "    # Flush any remaining messages in the producer buffer\n",
        "    producer.flush()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "WWBmPADiTEhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########################################################\n",
        ":"
      ],
      "metadata": {
        "id": "94uQkjrAUsSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Setting up a Kafka Consumer:\n",
        "   a) Write a Python program to create a Kafka consumer.\n",
        "   \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "KgPaHFkoSCqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "\n",
        "def consume_kafka_messages(consumer, topic):\n",
        "    consumer.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        msg = consumer.poll(1.0)  # Poll for new messages\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            print(f\"Consumer error: {msg.error()}\")\n",
        "            continue\n",
        "\n",
        "        # Process the received message\n",
        "        print(f\"Received message: {msg.value().decode('utf-8')}\")\n",
        "\n",
        "    consumer.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka consumer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'group.id': 'my_consumer_group',  # Consumer group ID for managing offsets\n",
        "        'auto.offset.reset': 'earliest'  # Start consuming from the beginning of the topic\n",
        "        # Add any other desired configuration options here\n",
        "    }\n",
        "\n",
        "    # Create Kafka consumer instance\n",
        "    consumer = Consumer(conf)\n",
        "\n",
        "    # Consume messages from Kafka topic\n",
        "    consume_kafka_messages(consumer, topic)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "9yM-ag74Sbkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2b) Configure the consumer to connect to a Kafka cluster.\n",
        "   \n"
      ],
      "metadata": {
        "id": "5X_zqlDWU2qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "\n",
        "def consume_kafka_messages(consumer, topic):\n",
        "    consumer.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        messages = consumer.consume(num_messages=5, timeout=1.0)  # Poll for new messages\n",
        "        for message in messages:\n",
        "            if message is None:\n",
        "                continue\n",
        "            if message.error():\n",
        "                print(f\"Consumer error: {message.error()}\")\n",
        "                continue\n",
        "\n",
        "            # Process the received message\n",
        "            print(f\"Received message: {message.value().decode('utf-8')}\")\n",
        "\n",
        "    consumer.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'kafka1:9092,kafka2:9092,kafka3:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka consumer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'group.id': 'my_consumer_group',\n",
        "        'auto.offset.reset': 'earliest'\n",
        "        # Add any other desired configuration options here\n",
        "    }\n",
        "\n",
        "    # Create Kafka consumer instance\n",
        "    consumer = Consumer(conf)\n",
        "\n",
        "    # Consume messages from Kafka topic\n",
        "    consume_kafka_messages(consumer, topic)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kJ8ku8lDU2ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " c) Implement logic to consume messages from a Kafka topic."
      ],
      "metadata": {
        "id": "wuz2QpPXU2HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "\n",
        "def consume_kafka_messages(consumer, topic):\n",
        "    consumer.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        messages = consumer.consume(num_messages=5, timeout=1.0)  # Poll for new messages\n",
        "        for message in messages:\n",
        "            if message is None:\n",
        "                continue\n",
        "            if message.error():\n",
        "                print(f\"Consumer error: {message.error()}\")\n",
        "                continue\n",
        "\n",
        "            # Process the received message\n",
        "            print(f\"Received message: {message.value().decode('utf-8')}\")\n",
        "\n",
        "        # Commit the consumed messages' offsets\n",
        "        consumer.commit()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'kafka1:9092,kafka2:9092,kafka3:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka consumer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'group.id': 'my_consumer_group',\n",
        "        'auto.offset.reset': 'earliest'\n",
        "        # Add any other desired configuration options here\n",
        "    }\n",
        "\n",
        "    # Create Kafka consumer instance\n",
        "    consumer = Consumer(conf)\n",
        "\n",
        "    # Consume messages from Kafka topic\n",
        "    consume_kafka_messages(consumer, topic)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "wiJY8AzVWkOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Creating and Managing Kafka Topics:\n",
        "   a) Write a Python program to create a new Kafka topic.\n",
        "   "
      ],
      "metadata": {
        "id": "5m_3_sIXScFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka.admin import AdminClient, NewTopic\n",
        "\n",
        "\n",
        "def create_kafka_topic(bootstrap_servers, topic, partitions=1, replication_factor=1):\n",
        "    # Create AdminClient configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers\n",
        "    }\n",
        "\n",
        "    # Create AdminClient instance\n",
        "    admin_client = AdminClient(conf)\n",
        "\n",
        "    # Create NewTopic object\n",
        "    new_topic = NewTopic(\n",
        "        topic,\n",
        "        num_partitions=partitions,\n",
        "        replication_factor=replication_factor\n",
        "    )\n",
        "\n",
        "    # Create the topic\n",
        "    admin_client.create_topics([new_topic])\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "    partitions = 3\n",
        "    replication_factor = 1\n",
        "\n",
        "    # Create the Kafka topic\n",
        "    create_kafka_topic(bootstrap_servers, topic, partitions, replication_factor)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "g0Qqjaw-YBY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Implement functionality to list existing topics.\n",
        "   \n",
        "   "
      ],
      "metadata": {
        "id": "o2GTkaTIYDf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka.admin import AdminClient\n",
        "\n",
        "\n",
        "def list_kafka_topics(bootstrap_servers):\n",
        "    # Create AdminClient configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers\n",
        "    }\n",
        "\n",
        "    # Create AdminClient instance\n",
        "    admin_client = AdminClient(conf)\n",
        "\n",
        "    # Get the list of existing topics\n",
        "    topics_metadata = admin_client.list_topics().topics\n",
        "\n",
        "    # Print the list of topics\n",
        "    print(\"Existing Topics:\")\n",
        "    for topic, metadata in topics_metadata.items():\n",
        "        print(f\"- {topic}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "\n",
        "    # List the Kafka topics\n",
        "    list_kafka_topics(bootstrap_servers)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0z04GQCPYDIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 c) Develop logic to delete an existing Kafka topic.\n"
      ],
      "metadata": {
        "id": "PaTAUbkmYCj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka.admin import AdminClient\n",
        "\n",
        "\n",
        "def delete_kafka_topic(bootstrap_servers, topic):\n",
        "    # Create AdminClient configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers\n",
        "    }\n",
        "\n",
        "    # Create AdminClient instance\n",
        "    admin_client = AdminClient(conf)\n",
        "\n",
        "    # Delete the topic\n",
        "    admin_client.delete_topics([topic])\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Delete the Kafka topic\n",
        "    delete_kafka_topic(bootstrap_servers, topic)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "efjCr97sSerN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#############################"
      ],
      "metadata": {
        "id": "ac2TQw_TlQ_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Producing and Consuming Messages:\n",
        "   a) Write a Python program to produce messages to a Kafka topic.\n"
      ],
      "metadata": {
        "id": "NJSezPU5Sfhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer\n",
        "\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    if err is not None:\n",
        "        print(f\"Message delivery failed: {err}\")\n",
        "    else:\n",
        "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
        "\n",
        "\n",
        "def produce_kafka_message(producer, topic, message):\n",
        "    producer.produce(topic, message.encode('utf-8'), callback=delivery_report)\n",
        "    producer.poll(0.5)  # Flushes the producer buffer\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka producer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers\n",
        "    }\n",
        "\n",
        "    # Create Kafka producer instance\n",
        "    producer = Producer(conf)\n",
        "\n",
        "    # Produce some messages\n",
        "    messages = [\n",
        "        'Hello, Kafka!',\n",
        "        'This is a test message.',\n",
        "        'Another message for Kafka.'\n",
        "    ]\n",
        "\n",
        "    for message in messages:\n",
        "        produce_kafka_message(producer, topic, message)\n",
        "\n",
        "    # Flush any remaining messages in the producer buffer\n",
        "    producer.flush()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "H6JMAVT4k-Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   b) Implement logic to consume messages from the same Kafka topic.\n",
        "  "
      ],
      "metadata": {
        "id": "Vuu3dZ_Qk_zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "\n",
        "def consume_kafka_messages(consumer, topic):\n",
        "    consumer.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        msg = consumer.poll(1.0)  # Poll for new messages\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            print(f\"Consumer error: {msg.error()}\")\n",
        "            continue\n",
        "\n",
        "        # Process the received message\n",
        "        print(f\"Received message: {msg.value().decode('utf-8')}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "\n",
        "    # Create Kafka consumer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'group.id': 'my_consumer_group',\n",
        "        'auto.offset.reset': 'earliest'\n",
        "        # Add any other desired configuration options here\n",
        "    }\n",
        "\n",
        "    # Create Kafka consumer instance\n",
        "    consumer = Consumer(conf)\n",
        "\n",
        "    # Consume messages from Kafka topic\n",
        "    consume_kafka_messages(consumer, topic)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "yFwulTqlk_cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " c) Test the end-to-end flow of message production and consumption.\n"
      ],
      "metadata": {
        "id": "dsm9eHdMk_Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Enter a message (or 'exit' to quit): Hello, Kafka!\n",
        "Enter a message (or 'exit' to quit): This is a test message.\n",
        "Enter a message (or 'exit' to quit): Another message for Kafka.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "mHllwbWElG55",
        "outputId": "f90eac21-a2fc-43c0-ea82-2e2bbc34204f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d84d8bd37f24>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Enter a message (or 'exit' to quit): Hello, Kafka!\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zTvtOCQClHbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "5. Working with Kafka Consumer Groups:\n",
        "   a) Write a Python program to create a Kafka consumer within a consumer group.\n",
        "\n"
      ],
      "metadata": {
        "id": "UMdSplScSpwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  from confluent_kafka import Consumer\n",
        "\n",
        "\n",
        "def consume_kafka_messages(consumer, topic):\n",
        "    consumer.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        messages = consumer.consume(num_messages=5, timeout=1.0)  # Poll for new messages\n",
        "        for message in messages:\n",
        "            if message is None:\n",
        "                continue\n",
        "            if message.error():\n",
        "                print(f\"Consumer error: {message.error()}\")\n",
        "                continue\n",
        "\n",
        "            # Process the received message\n",
        "            print(f\"Received message: {message.value().decode('utf-8')}\")\n",
        "\n",
        "        # Commit the consumed messages' offsets\n",
        "        consumer.commit()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "    group_id = 'my_consumer_group'\n",
        "\n",
        "    # Create Kafka consumer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'group.id': group_id,\n",
        "        'auto.offset.reset': 'earliest'\n",
        "        # Add any other desired configuration options here\n",
        "    }\n",
        "\n",
        "    # Create Kafka consumer instance\n",
        "    consumer = Consumer(conf)\n",
        "\n",
        "    # Consume messages from Kafka topic within the consumer group\n",
        "    consume_kafka_messages(consumer, topic)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "jszRrXEbSn6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " b) Implement logic to handle messages consumed by different consumers within the same group.\n",
        "    "
      ],
      "metadata": {
        "id": "JqsNlJQVmnhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer\n",
        "\n",
        "\n",
        "def consume_kafka_messages(consumer, topic):\n",
        "    consumer.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        messages = consumer.consume(num_messages=5, timeout=1.0)  # Poll for new messages\n",
        "        for message in messages:\n",
        "            if message is None:\n",
        "                continue\n",
        "            if message.error():\n",
        "                print(f\"Consumer error: {message.error()}\")\n",
        "                continue\n",
        "\n",
        "            # Process the received message\n",
        "            print(f\"Consumer {consumer.consumer_id()} - Received message: {message.value().decode('utf-8')}\")\n",
        "\n",
        "        # Commit the consumed messages' offsets\n",
        "        consumer.commit()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Kafka broker configuration\n",
        "    bootstrap_servers = 'localhost:9092'\n",
        "    topic = 'my_topic'\n",
        "    group_id = 'my_consumer_group'\n",
        "\n",
        "    # Create Kafka consumer configuration\n",
        "    conf = {\n",
        "        'bootstrap.servers': bootstrap_servers,\n",
        "        'group.id': group_id,\n",
        "        'auto.offset.reset': 'earliest'\n",
        "        # Add any other desired configuration options here\n",
        "    }\n",
        "\n",
        "    # Create Kafka consumer instance\n",
        "    consumer = Consumer(conf)\n",
        "\n",
        "    # Consume messages from Kafka topic within the consumer group\n",
        "    consume_kafka_messages(consumer, topic)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "rP73NjlzmnMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Observe the behavior of consumer group rebalancing when adding or removing consumers."
      ],
      "metadata": {
        "id": "1IcUZj8gmsmq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "964KweJgoOwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we add or remove consumers from a Kafka consumer group, it triggers a process called rebalancing. Rebalancing is the mechanism by which Kafka redistributes the partitions among the active consumers in the group to maintain balanced workload and ensure that each partition is consumed by only one consumer at a time. Here's how you can observe the behavior of consumer group rebalancing when adding or removing consumers:\n",
        "\n",
        "Start the Kafka cluster: Making sure Kafka cluster is running and accessible.\n",
        "\n",
        "Start the initial set of consumers: Run the consumer program (as described earlier) with a specified group.id and observe the messages being consumed by the consumers.\n",
        "\n",
        "Add a new consumer: Start another instance of the consumer program with the same group.id as the existing consumers. Observe that the new consumer joins the group and starts consuming messages from some partitions. The consumer group rebalances to distribute the partitions among the active consumers.\n",
        "\n",
        "Observe rebalancing behavior: During the rebalancing process, you will see log messages indicating the partition assignments and revocations. The rebalancing may take a few moments to complete, and each consumer's delivery_report callback may be called with err set to kafka.KafkaError._PARTITION_EOF. This indicates that the partitions being revoked are no longer assigned to that consumer.\n",
        "\n",
        "Verify consumption behavior: After the rebalancing process completes, observe that the consumers have been assigned different partitions, and each partition is consumed by only one consumer. You will see the respective consumer IDs and the consumed messages in the output.\n",
        "\n",
        "Remove a consumer: Stop one of the consumers that were previously running. Observe that the consumer group rebalances again to redistribute the partitions among the remaining consumers.\n",
        "\n",
        "Observe rebalancing behavior: During the rebalancing process triggered by the consumer's removal, you will again see log messages indicating the partition assignments and revocations. The remaining consumers will be assigned the partitions that were previously consumed by the stopped consumer.\n",
        "\n",
        "Verify consumption behavior: Once the rebalancing process completes, observe that the remaining consumers have new partition assignments, and each partition is consumed by only one consumer. The respective consumer IDs and consumed messages will be displayed in the output.\n",
        "\n",
        "By adding or removing consumers while the consumer group is active, you can observe how Kafka rebalances the partitions among the consumers to ensure a balanced workload distribution.\n",
        "\n",
        "Making sure have the necessary dependencies (confluent-kafka library) installed and"
      ],
      "metadata": {
        "id": "eYb3loH1oSar"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGXWVxY3omIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}